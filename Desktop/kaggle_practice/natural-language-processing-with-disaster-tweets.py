# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import re
print("Processing <å‡¦ç†ä¸­>...")
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
train = pd.read_csv("../input/nlp-getting-started/train.csv")
test = pd.read_csv("../input/nlp-getting-started/test.csv")

## kwã®æ¬ æå€¤å¯¾å‡¦
train = train.fillna({"keyword":0,
                     "location":0})
test = test.fillna({"keyword":0,
                   "location":0})
                   
# train_id = train["id"]
# train_kw = train["keyword"]
# train_loc = train["location"]
# train_txt = train["text"]
# train_tgt = train["target"]

# test_id = train["id"]
# test_kw = train["keyword"]
# test_loc = train["location"]
# test_txt = train["text"]

test_id = test["id"]
test_kw = test["keyword"]
# test_loc = test["location"] # ä½¿ç”¨ã—ãªã‹ã£ãŸ
# test_txt = test["text"] # ä½¿ç”¨ã—ãªã‹ã£ãŸ
                   

# print("freq : \n", keyword_df.value_counts())
keyword_dic = {}
kw_total_dic = {}
num = 1
for word, id_, add_score in zip(train["keyword"], train["id"], train["target"]):
    ## å‰²åˆè¨ˆç®—ã®ãŸã‚ã®ç·æ•°ï¼ˆåˆ†æ¯ï¼‰
    if word != 0: # æ¬ æå€¤Nanã‚’0ã«ã—ãŸã®ã§ã€ãã‚Œã‚’ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹
        try:
            kw_total_dic[word] += num
        except KeyError:
            kw_total_dic[word] = num

        ## å‰²åˆè¨ˆç®—ã®ãŸã‚ã®å‡ºç¾å›æ•°ï¼ˆåˆ†å­ï¼‰
        if add_score == 1:
            try:
                keyword_dic[word] += add_score
            except KeyError:
                keyword_dic[word] = add_score

# kw_digital_dic = {key: 0 for key in kw_total_dic.keys()}
kw_digital_dic = {}
for word in train["keyword"]:
    if word != 0:
        ## keyword_dic[word]ã«ã¯å­˜åœ¨ã—ãªã„ãŒkw_total_dic[num]ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®åŒå£«ã®æ¼”ç®—ã€€ã¨ã„ã†çŸ›ç›¾ã®ä¾‹å¤–å‡¦ç†
        try:
            kw_digital_dic[word] = round(keyword_dic[word] / kw_total_dic[word], 3) # å‰²åˆã‚’æ±‚ã‚ã‚‹
    #         kw_debug_dic = kw_digital_dic.copy() # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯é–“ã®ifã¨elseã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚Œã°å‰²åˆã‚’è¦‹ã‚Œã‚‰ã‚Œã‚‹ã€‚
            ## æ­£è§£ç‡30%ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ä¿å­˜ or æœªæº€ã¯0æ‰±ã„
            if round(keyword_dic[word] / kw_total_dic[word], 3) < 0.4:
                kw_digital_dic[word] = -1
    #           del kw_digital_dic[word] #æ¶ˆã—ãŸã‚‰å¾Œã§æ›¸ãè¾¼ã‚€ã¨ãã«ä¸ä¾¿ã‹ã‚‚
            else:
                kw_digital_dic[word] = 1 # ä»»æ„ã®å‰²åˆã‚’è¶ŠãˆãŸå ´åˆã€ãƒãƒªãƒ¥ãƒ¼ã‚’1ã«ã™ã‚‹ã€‚

        except KeyError:
            kw_digital_dic[word] = -1


# print("digital:\n", kw_digital_dic)
# print("kw_debug_dic: \n", kw_debug_dic)


# å†…åŒ…è¡¨è¨˜ã¯å·¦å´ã®forã‹ã‚‰å‡¦ç†ã•ã‚Œã‚‹ã€‚ãŸã ã—ã€ä¸€ç•ªå·¦ã¯æœ€å¾Œãªã®ã«æ³¨æ„ã€‚
# remove_lst = ["the", "that", "this", "with", "like", "from", "have", "&amp;", ] # ãã®ä»–ã€5h1hãªã©.æ­£è¦è¡¨ç¾ã§å¤§æ–‡å­—ã‚’å…¨éƒ¨å°æ–‡å­—ã«ã™ã‚‹ğŸš©
# word = [word for sentence in train["text"] for word in sentence.split() if (len(word)) >= 4 and (word not in remove_lst)] # 4æ–‡å­—ä»¥ä¸Šã‚’å–ã‚Šå‡ºã™ã€‚
# attention_lst = ["disaster", "catastrophe"]
# ã¨ã‚Šã‚ãˆãšå½¢æ…‹ç´ è§£æã§åè©ã ã‘æŠ½å‡ºã—ã¦ã¿ã‚‹ã€‚æ¬¡ã¯æ„Ÿæƒ…ã€å‹•è©ã¨ã‹ï¼Ÿ


## ğŸš©target=1ã®ã¨ãé »åº¦ãŒå¤§ãã„å˜èªã‚’ç½å®³æŒ‡å®šå˜èªã¨ã™ã‚‹ã€‚locationãªã©ã¨å…±é€šé›†åˆã¨ãªã£ãŸã‚‰ãƒ‡ãƒ¼ã‚¿é™¤å»ã®ä¸‹é™ã‚’ç·©å’Œã™ã‚‹ã€‚
loc_num_dic = {} # locã¯æ–‡å­—åˆ—å‹
id_loc_dic = {} # locã¯ãƒªã‚¹ãƒˆå‹
# åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
separators_lst = [",", "-", "|", "||", "&", "/", "!", "?"]
num = 0
for loc, id_, add_score in zip(train["location"], test_id, train["target"]):
    if (loc != 0) and (add_score == 1) and (loc != re.search("\d+", loc)):
        loc = loc.lower().replace(".", "")
    
        # æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸæ–‡å­—åˆ—ã‚’å‰Šé™¤
        loc = re.sub(r'\([^)]*\)', '', loc)

        ## æ­£è¦è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
        pattern = "|".join(map(re.escape, separators_lst))
        # re.escape(æ–‡å­—åˆ—)ã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–ã‚Šæ‰±ã†ãŸã‚ã®å‡¦ç†ã€‚separatorsã¯ãƒªã‚¹ãƒˆãªã®ã§ç›´æ¥ã¯ä½¿ãˆãªã„ã€‚
        # map(é–¢æ•°, ãƒªã‚¹ãƒˆorã‚¤ãƒ†ãƒ©ãƒ–ãƒ«)ã§ã€é–¢æ•°ã‚’ãƒªã‚¹ãƒˆç­‰ã«é©ç”¨ã•ã›ãŸæ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã€‚
        
        ## æ­£è¦è¡¨ç¾ã§åŒºåˆ‡ã‚Šæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if re.search(pattern, loc):
            parts_lst = re.split(pattern, loc)
            for loc_part in parts_lst:
                
                ## loc_partãŒå…¨ã¦æ•°å­—ã®å ´åˆã‚’é™¤ããŸã‚ã®ä¾‹å¤–å‡¦ç†ã€‚
                try:
                    if int(loc_part):
                        pass
                    
                except ValueError:
                    ## æ–‡é ­ã¨æ–‡æœ«ã®ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
                    if loc_part != "":
                        if loc_part[0] == " ":
                            loc_part = loc_part[1:]
                    if len(loc_part) > 1:
                        if loc_part[-1] == " ":
                            loc_part = loc_part[:-1]
                            
                    ## "å ´æ‰€ï¼ˆãƒªã‚¹ãƒˆã§ã¯ãªã„ï¼‰"ãƒ»"å ´æ‰€ã®æ•°"ã®ãƒšã‚¢ã§ä¿å­˜ã™ã‚‹è¾æ›¸ã«ã‚­ãƒ¼ãŒã™ã§ã«ã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã®ä¾‹å¤–å‡¦ç†
                    try:
                        loc_num_dic[loc_part] += add_score
                    except KeyError:
                        loc_num_dic[loc_part] = add_score
                        
                    if loc_part != '': # ãªã‚“ã‹loc_partãŒ''ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§é™¤å»
                        ## "id"ãƒ»"å ´æ‰€ï¼ˆãƒªã‚¹ãƒˆï¼‰"ã®ãƒšã‚¢ã§ä¿å­˜ã™ã‚‹è¾æ›¸ã«ã‚­ãƒ¼ãŒã™ã§ã«ã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã®ä¾‹å¤–å‡¦ç†
                        try:
                                id_loc_dic[id_].append(loc_part)
                        except KeyError:
                            id_loc_dic[id_] = [] # ãƒãƒªãƒ¥ãƒ¼ï¼ˆå€‹ã€…ã®locï¼‰ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
                            id_loc_dic[id_].append(loc_part)
        else:
            try:    
                loc_num_dic[loc] += add_score
            except KeyError:
                loc_num_dic[loc] = add_score                            
            try:
                id_loc_dic[id_].append(loc)
            except KeyError:
                id_loc_dic[id_] = []
                id_loc_dic[id_].append(loc)

# print("loc_num_dic:\n", loc_num_dic)
# print("id_loc_dic: ", id_loc_dic)

loc_num_dic = {loc: 1 if num >= 2 else -1 for loc, num in loc_num_dic.items()} # æŒ‡å®šå›æ•°ä»¥ä¸Šå‡ºç¾ã—ãŸã‚‰,numã‚’1ã«æ›´æ–°ã€‚ãã†ã˜ã‚ƒãªã‹ã£ãŸã‚‰-1ã«æ›´æ–°ã€‚ä»Šã¯2å›ä»¥ä¸Šå‡ºç¾ã—ã¦ã„ã‚‹å ´æ‰€(location)ã«é™ã£ã¦ã„ã‚‹ã€‚
        
id_loc_digital_dic = dict.fromkeys(test_id, 0) # ã‚­ãƒ¼ã ã‘ï¼ˆidã ã‘ï¼‰ã‚­ãƒ¼ã‚’æµç”¨ã—ãŸè¾æ›¸ä½œæˆã€‚åˆæœŸãƒãƒªãƒ¥ãƒ¼ã¯ç¬¬äºŒå¼•æ•°0ã«è¨­å®šã€‚    
## idã¨0,1å€¤ã‚’å¯¾å¿œã•ã›ã‚‹
for id_, loc_lst in id_loc_dic.items():
    for loc in loc_lst:
        if loc_num_dic[loc] == 1:
            ## idã¨ãã®numã‚’ãƒšã‚¢ã«æŒã¤è¾æ›¸ã«1ã‚’ä¿å­˜ã€‚id_loc_digital_dicã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªãƒ¥ãƒ¼ã‚’0ã«è¨­å®šã—ãŸã®ã§ã€å«ã¾ã‚Œã¦ã„ãŸã‚‰1ã«æ›´æ–°ã™ã‚‹ã ã‘ã§è‰¯ã„ã€‚
            id_loc_digital_dic[id_] = 1 
        elif loc_num_dic[loc] == -1:
            id_loc_digital_dic[id_] = -1 # é »åº¦ãŒå°ã•ãã¦åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚ŒãŸå ´æ‰€(loc)ã‚’æ ¼ç´ã€‚

# print("id_loc_dic(digital):", id_loc_dic)
# print("id_num_dic:", id_num_dic)
# print("id_len: ", len(test_id))
# print("dic_len: ", len(id_loc_dic))
# print("id_loc_digital_dic:", id_loc_digital_dic)


import nltk
from nltk.tokenize import word_tokenize

text_noun_dic = {}
id_word_dic = {id_: [] for id_ in test_id} # ã‚ã‚‰ã‹ã˜ã‚ç©ºãƒªã‚¹ãƒˆã‚’åˆæœŸå€¤ã¨ã—ã¦ä½œã£ã¦ãŠã„ãŸ
# print("###", id_word_dic)
no_need_lst = ["http", "https", "amp", "@"] # ğŸš©
for id_, sentence, add_score in zip(test_id, train["text"], train["target"]):
    tokens = nltk.word_tokenize(sentence)
    tagged_lst = nltk.pos_tag(tokens)
    
    for word, pos in tagged_lst:
        # åè©,ã¤ã¾ã‚ŠNNã®ã¿å–å¾—
        if (pos == "NN") and (word not in no_need_lst) and (add_score != 0):
            try:
                text_noun_dic[word] += add_score # row_idxã¨wordã‚’ç´ã¥ã‘ãŸã„...è¾æ›¸å‹ã«ã™ã‚‹ã¨ã‹ï¼Ÿ
            except KeyError:
                text_noun_dic[word] = add_score
                
            ## idã¨å˜èªã®çµ„ã¿åˆã‚ã›ã‚’ä¿å­˜
            id_word_dic[id_].append(word)

text_noun_dic = {word: 1 if num >= 2 else -1 for word, num in text_noun_dic.items()} # é »åº¦ã®å°ã•ã„å˜èªã®åˆ‡ã‚Šæ¨ã¦.ä»Šã¯å–ã‚Šåˆãˆãš2ä»¥ä¸Šã«ã—ã¦ã‚‹

id_word_digital_dic = dict.fromkeys(test_id, 0)
## idã¨0,1ã¨ã‚’å¯¾å¿œã•ã›ã‚‹
for id_, word_lst in id_word_dic.items():
# for id_, word_lst in zip(test_id, id_word_dic.values()):
    for word in word_lst:
        if text_noun_dic[word] == 1:
            id_word_digital_dic[id_] = 1
        elif text_noun_dic[word] == -1:
            id_word_digital_dic[id_] = -1


## å‰å‡¦ç†å¾Œã®ãƒªã‚¹ãƒˆ
kw_digital_lst = [kw_digital_dic.get(kw, 0) for kw in test_kw] # kwãŒå­˜åœ¨ã™ã‚Œã°ãã®å€¤ã‚’è¿”ã—ã€å­˜åœ¨ã—ãªã‘ã‚Œã°æ¬ æå€¤ã¨ã—ã¦0ã‚’è¿”ã—ã¦ã„ã‚‹ã€‚-1ã¯åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚
loc_digital_lst = list(id_loc_digital_dic.values())
word_digital_lst = list(id_word_digital_dic.values())
# print("id: ", len(test_id))
# print("kw: ", len(kw_digital_lst))
# print("loc: ", len(loc_digital_lst))
# print("text: ", len(word_digital_lst))


## ç¢ºèªdebugç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
# preprocessing = pd.DataFrame(
#                     data = {"<id>": test_id,
#                             "<keyword>": kw_digital_lst,
#                            "<location>": loc_digital_lst,
#                            "<text>": word_digital_lst,
#                             "true_target":train_tgt
#                            }
# )
# print("ğŸš©preprocessing_dataFrame:\n", preprocessing[30:50])

prepared_lst = []

for kw, loc, word in zip(kw_digital_lst, loc_digital_lst, word_digital_lst):
    ## kwã®åˆ¤å®š
    if kw != -1 and word != -1 and (kw != 0 or word != 0): # kwã¨wordãŒåˆ‡ã‚Šæ¨ã¦å€¤ã§ã¯ãªã„ã‹ã¤æ¬ æå€¤ã§ã‚‚ãªã„å ´åˆ
        prepared_lst.append(1)
    ## locã®åˆ¤å®š
    elif loc: # loc=1ã®ã¨ã
        prepared_lst.append(1)
    ## wordã®åˆ¤å®š
    elif kw == 0 and loc == 0 and word == 1: # ä¸¡æ–¹ã¨ã‚‚æ¬ æå€¤ã®ã¨ãã€wordãŒ1ãªã‚‰ã°
        prepared_lst.append(1)
    else:
        prepared_lst.append(0)

finalize_df = pd.DataFrame({
                    "id":test_id,
                    "target":prepared_lst
})
print("finalize_df:", finalize_df)

finalize_df.to_csv("disaster_tweet.csv", index = False)


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
## 32ç•ªç›®ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å ´æ‰€
## targetã¯0ã‹ã‚‰14è¡Œ(id=20)ä»¥é™0
## ğŸš©textåçœã€€ç²¾åº¦ãŒæ‚ªã„ã€‚åè©ã®ã¿ã§ã‚„ã£ã¦ã¿ãŸãŒã€flood,floodedã‚„floodingã¯ä½•åˆ¤å®šï¼Ÿå‹•è©ï¼Ÿ å½¢æ…‹ç´ è§£æã«è¿½åŠ ã—ãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã€‚
### é‡ã¿ä»˜ã‘åŸºæº–ï¼šã€€location > keyword > text
### locationãŒã‚ã£ãŸã‚‰å•ç­”ç„¡ç”¨ã§true, keywordã¯textã‚‚1ãªã‚‰true, textã¯ãã‚Œå˜ä½“ã®ã¨ãã®ã¿(kw,locãŒ0ã®ã¨ã)æ¡ç”¨
print("Process complete <ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†>")

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session