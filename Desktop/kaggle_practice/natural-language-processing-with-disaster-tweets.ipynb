{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f6c1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T08:26:28.610844Z",
     "iopub.status.busy": "2023-10-15T08:26:28.609998Z",
     "iopub.status.idle": "2023-10-15T08:26:35.396087Z",
     "shell.execute_reply": "2023-10-15T08:26:35.394804Z"
    },
    "papermill": {
     "duration": 6.793353,
     "end_time": "2023-10-15T08:26:35.398825",
     "exception": false,
     "start_time": "2023-10-15T08:26:28.605472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing <å‡¦ç†ä¸­>...\n",
      "finalize_df:          id  target\n",
      "0         0       1\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       0\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       0\n",
      "3259  10865       0\n",
      "3260  10868       1\n",
      "3261  10874       0\n",
      "3262  10875       0\n",
      "\n",
      "[3263 rows x 2 columns]\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "Process complete <ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†>\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import re\n",
    "print(\"Processing <å‡¦ç†ä¸­>...\")\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "\n",
    "## kwã®æ¬ æå€¤å¯¾å‡¦\n",
    "train = train.fillna({\"keyword\":0,\n",
    "                     \"location\":0})\n",
    "test = test.fillna({\"keyword\":0,\n",
    "                   \"location\":0})\n",
    "                   \n",
    "# train_id = train[\"id\"]\n",
    "# train_kw = train[\"keyword\"]\n",
    "# train_loc = train[\"location\"]\n",
    "# train_txt = train[\"text\"]\n",
    "# train_tgt = train[\"target\"]\n",
    "\n",
    "# test_id = train[\"id\"]\n",
    "# test_kw = train[\"keyword\"]\n",
    "# test_loc = train[\"location\"]\n",
    "# test_txt = train[\"text\"]\n",
    "\n",
    "test_id = test[\"id\"]\n",
    "test_kw = test[\"keyword\"]\n",
    "# test_loc = test[\"location\"] # ä½¿ç”¨ã—ãªã‹ã£ãŸ\n",
    "# test_txt = test[\"text\"] # ä½¿ç”¨ã—ãªã‹ã£ãŸ\n",
    "                   \n",
    "\n",
    "# print(\"freq : \\n\", keyword_df.value_counts())\n",
    "keyword_dic = {}\n",
    "kw_total_dic = {}\n",
    "num = 1\n",
    "for word, id_, add_score in zip(train[\"keyword\"], train[\"id\"], train[\"target\"]):\n",
    "    ## å‰²åˆè¨ˆç®—ã®ãŸã‚ã®ç·æ•°ï¼ˆåˆ†æ¯ï¼‰\n",
    "    if word != 0: # æ¬ æå€¤Nanã‚’0ã«ã—ãŸã®ã§ã€ãã‚Œã‚’ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹\n",
    "        try:\n",
    "            kw_total_dic[word] += num\n",
    "        except KeyError:\n",
    "            kw_total_dic[word] = num\n",
    "\n",
    "        ## å‰²åˆè¨ˆç®—ã®ãŸã‚ã®å‡ºç¾å›æ•°ï¼ˆåˆ†å­ï¼‰\n",
    "        if add_score == 1:\n",
    "            try:\n",
    "                keyword_dic[word] += add_score\n",
    "            except KeyError:\n",
    "                keyword_dic[word] = add_score\n",
    "\n",
    "# kw_digital_dic = {key: 0 for key in kw_total_dic.keys()}\n",
    "kw_digital_dic = {}\n",
    "for word in train[\"keyword\"]:\n",
    "    if word != 0:\n",
    "        ## keyword_dic[word]ã«ã¯å­˜åœ¨ã—ãªã„ãŒkw_total_dic[num]ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®åŒå£«ã®æ¼”ç®—ã€€ã¨ã„ã†çŸ›ç›¾ã®ä¾‹å¤–å‡¦ç†\n",
    "        try:\n",
    "            kw_digital_dic[word] = round(keyword_dic[word] / kw_total_dic[word], 3) # å‰²åˆã‚’æ±‚ã‚ã‚‹\n",
    "    #         kw_debug_dic = kw_digital_dic.copy() # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯é–“ã®ifã¨elseã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚Œã°å‰²åˆã‚’è¦‹ã‚Œã‚‰ã‚Œã‚‹ã€‚\n",
    "            ## æ­£è§£ç‡30%ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ä¿å­˜ or æœªæº€ã¯0æ‰±ã„\n",
    "            if round(keyword_dic[word] / kw_total_dic[word], 3) < 0.4:\n",
    "                kw_digital_dic[word] = -1\n",
    "    #           del kw_digital_dic[word] #æ¶ˆã—ãŸã‚‰å¾Œã§æ›¸ãè¾¼ã‚€ã¨ãã«ä¸ä¾¿ã‹ã‚‚\n",
    "            else:\n",
    "                kw_digital_dic[word] = 1 # ä»»æ„ã®å‰²åˆã‚’è¶ŠãˆãŸå ´åˆã€ãƒãƒªãƒ¥ãƒ¼ã‚’1ã«ã™ã‚‹ã€‚\n",
    "\n",
    "        except KeyError:\n",
    "            kw_digital_dic[word] = -1\n",
    "\n",
    "\n",
    "# print(\"digital:\\n\", kw_digital_dic)\n",
    "# print(\"kw_debug_dic: \\n\", kw_debug_dic)\n",
    "\n",
    "\n",
    "# å†…åŒ…è¡¨è¨˜ã¯å·¦å´ã®forã‹ã‚‰å‡¦ç†ã•ã‚Œã‚‹ã€‚ãŸã ã—ã€ä¸€ç•ªå·¦ã¯æœ€å¾Œãªã®ã«æ³¨æ„ã€‚\n",
    "# remove_lst = [\"the\", \"that\", \"this\", \"with\", \"like\", \"from\", \"have\", \"&amp;\", ] # ãã®ä»–ã€5h1hãªã©.æ­£è¦è¡¨ç¾ã§å¤§æ–‡å­—ã‚’å…¨éƒ¨å°æ–‡å­—ã«ã™ã‚‹ğŸš©\n",
    "# word = [word for sentence in train[\"text\"] for word in sentence.split() if (len(word)) >= 4 and (word not in remove_lst)] # 4æ–‡å­—ä»¥ä¸Šã‚’å–ã‚Šå‡ºã™ã€‚\n",
    "# attention_lst = [\"disaster\", \"catastrophe\"]\n",
    "# ã¨ã‚Šã‚ãˆãšå½¢æ…‹ç´ è§£æã§åè©ã ã‘æŠ½å‡ºã—ã¦ã¿ã‚‹ã€‚æ¬¡ã¯æ„Ÿæƒ…ã€å‹•è©ã¨ã‹ï¼Ÿ\n",
    "\n",
    "\n",
    "## ğŸš©target=1ã®ã¨ãé »åº¦ãŒå¤§ãã„å˜èªã‚’ç½å®³æŒ‡å®šå˜èªã¨ã™ã‚‹ã€‚locationãªã©ã¨å…±é€šé›†åˆã¨ãªã£ãŸã‚‰ãƒ‡ãƒ¼ã‚¿é™¤å»ã®ä¸‹é™ã‚’ç·©å’Œã™ã‚‹ã€‚\n",
    "loc_num_dic = {} # locã¯æ–‡å­—åˆ—å‹\n",
    "id_loc_dic = {} # locã¯ãƒªã‚¹ãƒˆå‹\n",
    "# åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "separators_lst = [\",\", \"-\", \"|\", \"||\", \"&\", \"/\", \"!\", \"?\"]\n",
    "num = 0\n",
    "for loc, id_, add_score in zip(train[\"location\"], test_id, train[\"target\"]):\n",
    "    if (loc != 0) and (add_score == 1) and (loc != re.search(\"\\d+\", loc)):\n",
    "        loc = loc.lower().replace(\".\", \"\")\n",
    "    \n",
    "        # æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸæ–‡å­—åˆ—ã‚’å‰Šé™¤\n",
    "        loc = re.sub(r'\\([^)]*\\)', '', loc)\n",
    "\n",
    "        ## æ­£è¦è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
    "        pattern = \"|\".join(map(re.escape, separators_lst))\n",
    "        # re.escape(æ–‡å­—åˆ—)ã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–ã‚Šæ‰±ã†ãŸã‚ã®å‡¦ç†ã€‚separatorsã¯ãƒªã‚¹ãƒˆãªã®ã§ç›´æ¥ã¯ä½¿ãˆãªã„ã€‚\n",
    "        # map(é–¢æ•°, ãƒªã‚¹ãƒˆorã‚¤ãƒ†ãƒ©ãƒ–ãƒ«)ã§ã€é–¢æ•°ã‚’ãƒªã‚¹ãƒˆç­‰ã«é©ç”¨ã•ã›ãŸæ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã€‚\n",
    "        \n",
    "        ## æ­£è¦è¡¨ç¾ã§åŒºåˆ‡ã‚Šæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "        if re.search(pattern, loc):\n",
    "            parts_lst = re.split(pattern, loc)\n",
    "            for loc_part in parts_lst:\n",
    "                \n",
    "                ## loc_partãŒå…¨ã¦æ•°å­—ã®å ´åˆã‚’é™¤ããŸã‚ã®ä¾‹å¤–å‡¦ç†ã€‚\n",
    "                try:\n",
    "                    if int(loc_part):\n",
    "                        pass\n",
    "                    \n",
    "                except ValueError:\n",
    "                    ## æ–‡é ­ã¨æ–‡æœ«ã®ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤\n",
    "                    if loc_part != \"\":\n",
    "                        if loc_part[0] == \" \":\n",
    "                            loc_part = loc_part[1:]\n",
    "                    if len(loc_part) > 1:\n",
    "                        if loc_part[-1] == \" \":\n",
    "                            loc_part = loc_part[:-1]\n",
    "                            \n",
    "                    ## \"å ´æ‰€ï¼ˆãƒªã‚¹ãƒˆã§ã¯ãªã„ï¼‰\"ãƒ»\"å ´æ‰€ã®æ•°\"ã®ãƒšã‚¢ã§ä¿å­˜ã™ã‚‹è¾æ›¸ã«ã‚­ãƒ¼ãŒã™ã§ã«ã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã®ä¾‹å¤–å‡¦ç†\n",
    "                    try:\n",
    "                        loc_num_dic[loc_part] += add_score\n",
    "                    except KeyError:\n",
    "                        loc_num_dic[loc_part] = add_score\n",
    "                        \n",
    "                    if loc_part != '': # ãªã‚“ã‹loc_partãŒ''ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§é™¤å»\n",
    "                        ## \"id\"ãƒ»\"å ´æ‰€ï¼ˆãƒªã‚¹ãƒˆï¼‰\"ã®ãƒšã‚¢ã§ä¿å­˜ã™ã‚‹è¾æ›¸ã«ã‚­ãƒ¼ãŒã™ã§ã«ã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã®ä¾‹å¤–å‡¦ç†\n",
    "                        try:\n",
    "                                id_loc_dic[id_].append(loc_part)\n",
    "                        except KeyError:\n",
    "                            id_loc_dic[id_] = [] # ãƒãƒªãƒ¥ãƒ¼ï¼ˆå€‹ã€…ã®locï¼‰ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ\n",
    "                            id_loc_dic[id_].append(loc_part)\n",
    "        else:\n",
    "            try:    \n",
    "                loc_num_dic[loc] += add_score\n",
    "            except KeyError:\n",
    "                loc_num_dic[loc] = add_score                            \n",
    "            try:\n",
    "                id_loc_dic[id_].append(loc)\n",
    "            except KeyError:\n",
    "                id_loc_dic[id_] = []\n",
    "                id_loc_dic[id_].append(loc)\n",
    "\n",
    "# print(\"loc_num_dic:\\n\", loc_num_dic)\n",
    "# print(\"id_loc_dic: \", id_loc_dic)\n",
    "\n",
    "loc_num_dic = {loc: 1 if num >= 2 else -1 for loc, num in loc_num_dic.items()} # æŒ‡å®šå›æ•°ä»¥ä¸Šå‡ºç¾ã—ãŸã‚‰,numã‚’1ã«æ›´æ–°ã€‚ãã†ã˜ã‚ƒãªã‹ã£ãŸã‚‰-1ã«æ›´æ–°ã€‚ä»Šã¯2å›ä»¥ä¸Šå‡ºç¾ã—ã¦ã„ã‚‹å ´æ‰€(location)ã«é™ã£ã¦ã„ã‚‹ã€‚\n",
    "        \n",
    "id_loc_digital_dic = dict.fromkeys(test_id, 0) # ã‚­ãƒ¼ã ã‘ï¼ˆidã ã‘ï¼‰ã‚­ãƒ¼ã‚’æµç”¨ã—ãŸè¾æ›¸ä½œæˆã€‚åˆæœŸãƒãƒªãƒ¥ãƒ¼ã¯ç¬¬äºŒå¼•æ•°0ã«è¨­å®šã€‚    \n",
    "## idã¨0,1å€¤ã‚’å¯¾å¿œã•ã›ã‚‹\n",
    "for id_, loc_lst in id_loc_dic.items():\n",
    "    for loc in loc_lst:\n",
    "        if loc_num_dic[loc] == 1:\n",
    "            ## idã¨ãã®numã‚’ãƒšã‚¢ã«æŒã¤è¾æ›¸ã«1ã‚’ä¿å­˜ã€‚id_loc_digital_dicã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªãƒ¥ãƒ¼ã‚’0ã«è¨­å®šã—ãŸã®ã§ã€å«ã¾ã‚Œã¦ã„ãŸã‚‰1ã«æ›´æ–°ã™ã‚‹ã ã‘ã§è‰¯ã„ã€‚\n",
    "            id_loc_digital_dic[id_] = 1 \n",
    "        elif loc_num_dic[loc] == -1:\n",
    "            id_loc_digital_dic[id_] = -1 # é »åº¦ãŒå°ã•ãã¦åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚ŒãŸå ´æ‰€(loc)ã‚’æ ¼ç´ã€‚\n",
    "\n",
    "# print(\"id_loc_dic(digital):\", id_loc_dic)\n",
    "# print(\"id_num_dic:\", id_num_dic)\n",
    "# print(\"id_len: \", len(test_id))\n",
    "# print(\"dic_len: \", len(id_loc_dic))\n",
    "# print(\"id_loc_digital_dic:\", id_loc_digital_dic)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_noun_dic = {}\n",
    "id_word_dic = {id_: [] for id_ in test_id} # ã‚ã‚‰ã‹ã˜ã‚ç©ºãƒªã‚¹ãƒˆã‚’åˆæœŸå€¤ã¨ã—ã¦ä½œã£ã¦ãŠã„ãŸ\n",
    "# print(\"###\", id_word_dic)\n",
    "no_need_lst = [\"http\", \"https\", \"amp\", \"@\"] # ğŸš©\n",
    "for id_, sentence, add_score in zip(test_id, train[\"text\"], train[\"target\"]):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_lst = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for word, pos in tagged_lst:\n",
    "        # åè©,ã¤ã¾ã‚ŠNNã®ã¿å–å¾—\n",
    "        if (pos == \"NN\") and (word not in no_need_lst) and (add_score != 0):\n",
    "            try:\n",
    "                text_noun_dic[word] += add_score # row_idxã¨wordã‚’ç´ã¥ã‘ãŸã„...è¾æ›¸å‹ã«ã™ã‚‹ã¨ã‹ï¼Ÿ\n",
    "            except KeyError:\n",
    "                text_noun_dic[word] = add_score\n",
    "                \n",
    "            ## idã¨å˜èªã®çµ„ã¿åˆã‚ã›ã‚’ä¿å­˜\n",
    "            id_word_dic[id_].append(word)\n",
    "\n",
    "text_noun_dic = {word: 1 if num >= 2 else -1 for word, num in text_noun_dic.items()} # é »åº¦ã®å°ã•ã„å˜èªã®åˆ‡ã‚Šæ¨ã¦.ä»Šã¯å–ã‚Šåˆãˆãš2ä»¥ä¸Šã«ã—ã¦ã‚‹\n",
    "\n",
    "id_word_digital_dic = dict.fromkeys(test_id, 0)\n",
    "## idã¨0,1ã¨ã‚’å¯¾å¿œã•ã›ã‚‹\n",
    "for id_, word_lst in id_word_dic.items():\n",
    "# for id_, word_lst in zip(test_id, id_word_dic.values()):\n",
    "    for word in word_lst:\n",
    "        if text_noun_dic[word] == 1:\n",
    "            id_word_digital_dic[id_] = 1\n",
    "        elif text_noun_dic[word] == -1:\n",
    "            id_word_digital_dic[id_] = -1\n",
    "\n",
    "\n",
    "## å‰å‡¦ç†å¾Œã®ãƒªã‚¹ãƒˆ\n",
    "kw_digital_lst = [kw_digital_dic.get(kw, 0) for kw in test_kw] # kwãŒå­˜åœ¨ã™ã‚Œã°ãã®å€¤ã‚’è¿”ã—ã€å­˜åœ¨ã—ãªã‘ã‚Œã°æ¬ æå€¤ã¨ã—ã¦0ã‚’è¿”ã—ã¦ã„ã‚‹ã€‚-1ã¯åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚\n",
    "loc_digital_lst = list(id_loc_digital_dic.values())\n",
    "word_digital_lst = list(id_word_digital_dic.values())\n",
    "# print(\"id: \", len(test_id))\n",
    "# print(\"kw: \", len(kw_digital_lst))\n",
    "# print(\"loc: \", len(loc_digital_lst))\n",
    "# print(\"text: \", len(word_digital_lst))\n",
    "\n",
    "\n",
    "## ç¢ºèªdebugç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "# preprocessing = pd.DataFrame(\n",
    "#                     data = {\"<id>\": test_id,\n",
    "#                             \"<keyword>\": kw_digital_lst,\n",
    "#                            \"<location>\": loc_digital_lst,\n",
    "#                            \"<text>\": word_digital_lst,\n",
    "#                             \"true_target\":train_tgt\n",
    "#                            }\n",
    "# )\n",
    "# print(\"ğŸš©preprocessing_dataFrame:\\n\", preprocessing[30:50])\n",
    "\n",
    "prepared_lst = []\n",
    "\n",
    "for kw, loc, word in zip(kw_digital_lst, loc_digital_lst, word_digital_lst):\n",
    "    ## kwã®åˆ¤å®š\n",
    "    if kw != -1 and word != -1 and (kw != 0 or word != 0): # kwã¨wordãŒåˆ‡ã‚Šæ¨ã¦å€¤ã§ã¯ãªã„ã‹ã¤æ¬ æå€¤ã§ã‚‚ãªã„å ´åˆ\n",
    "        prepared_lst.append(1)\n",
    "    ## locã®åˆ¤å®š\n",
    "    elif loc: # loc=1ã®ã¨ã\n",
    "        prepared_lst.append(1)\n",
    "    ## wordã®åˆ¤å®š\n",
    "    elif kw == 0 and loc == 0 and word == 1: # ä¸¡æ–¹ã¨ã‚‚æ¬ æå€¤ã®ã¨ãã€wordãŒ1ãªã‚‰ã°\n",
    "        prepared_lst.append(1)\n",
    "    else:\n",
    "        prepared_lst.append(0)\n",
    "\n",
    "finalize_df = pd.DataFrame({\n",
    "                    \"id\":test_id,\n",
    "                    \"target\":prepared_lst\n",
    "})\n",
    "print(\"finalize_df:\", finalize_df)\n",
    "\n",
    "finalize_df.to_csv(\"disaster_tweet.csv\", index = False)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "## 32ç•ªç›®ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å ´æ‰€\n",
    "## targetã¯0ã‹ã‚‰14è¡Œ(id=20)ä»¥é™0\n",
    "## ğŸš©textåçœã€€ç²¾åº¦ãŒæ‚ªã„ã€‚åè©ã®ã¿ã§ã‚„ã£ã¦ã¿ãŸãŒã€flood,floodedã‚„floodingã¯ä½•åˆ¤å®šï¼Ÿå‹•è©ï¼Ÿ å½¢æ…‹ç´ è§£æã«è¿½åŠ ã—ãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã€‚\n",
    "### é‡ã¿ä»˜ã‘åŸºæº–ï¼šã€€location > keyword > text\n",
    "### locationãŒã‚ã£ãŸã‚‰å•ç­”ç„¡ç”¨ã§true, keywordã¯textã‚‚1ãªã‚‰true, textã¯ãã‚Œå˜ä½“ã®ã¨ãã®ã¿(kw,locãŒ0ã®ã¨ã)æ¡ç”¨\n",
    "print(\"Process complete <ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†>\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c51888",
   "metadata": {
    "papermill": {
     "duration": 0.001768,
     "end_time": "2023-10-15T08:26:35.404872",
     "exception": false,
     "start_time": "2023-10-15T08:26:35.403104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.745397,
   "end_time": "2023-10-15T08:26:36.128342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-15T08:26:25.382945",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
